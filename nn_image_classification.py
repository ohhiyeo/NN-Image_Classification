# -*- coding: utf-8 -*-
"""NN-Image_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kV3ytMx2CasGBpPuIkeNPaFTvz2L40wD
"""

#########################################################################################
#         BATCH_SIZE = 1:                                                               #
#         Epoch   0 Time     40.8 lr = 0.000100 avg loss = 0.352032 accuracy = 90.70    # 
#         Epoch   1 Time     40.8 lr = 0.003400 avg loss = 0.201859 accuracy = 94.94    #
#         Epoch   2 Time     40.6 lr = 0.006700 avg loss = 0.125801 accuracy = 95.78    #
#         Epoch   3 Time     40.6 lr = 0.010000 avg loss = 0.098636 accuracy = 95.21    #
#         Epoch   4 Time     40.8 lr = 0.009699 avg loss = 0.069400 accuracy = 96.73    #
#         Epoch   5 Time     40.6 lr = 0.008831 avg loss = 0.050098 accuracy = 96.99    #
#         Epoch   6 Time     40.8 lr = 0.007502 avg loss = 0.035275 accuracy = 97.57    #
#         Epoch   7 Time     40.6 lr = 0.005872 avg loss = 0.024438 accuracy = 97.52    #
#         Epoch   8 Time     40.7 lr = 0.004138 avg loss = 0.016617 accuracy = 97.76    #
#         Epoch   9 Time     40.6 lr = 0.002507 avg loss = 0.011746 accuracy = 98.00    #
#         Epoch  10 Time     40.6 lr = 0.001178 avg loss = 0.009105 accuracy = 98.05    #
#         Epoch  11 Time     40.6 lr = 0.000311 avg loss = 0.007955 accuracy = 98.05    #
#         Epoch  12 Time     40.9 lr = 0.000010 avg loss = 0.007389 accuracy = 98.06    #
#                                                                                       #
#         BATCH_SIZE = 32:                                                              #
#         Epoch   0 Time      6.5 lr = 0.000100 avg loss = 2.365506 accuracy =  6.26    #
#         Epoch   1 Time      6.6 lr = 0.003400 avg loss = 2.340736 accuracy =  7.51    #
#         Epoch   2 Time      6.5 lr = 0.006700 avg loss = 2.316888 accuracy = 10.47    #
#         Epoch   3 Time      6.7 lr = 0.010000 avg loss = 2.286211 accuracy = 16.54    #
#         Epoch   4 Time      6.6 lr = 0.009699 avg loss = 2.247941 accuracy = 22.72    #
#         Epoch   5 Time      6.5 lr = 0.008831 avg loss = 2.204488 accuracy = 29.74    #
#         Epoch   6 Time      6.5 lr = 0.007502 avg loss = 2.154590 accuracy = 35.92    #
#         Epoch   7 Time      6.4 lr = 0.005872 avg loss = 2.100983 accuracy = 40.18    #
#         Epoch   8 Time      6.6 lr = 0.004138 avg loss = 2.050878 accuracy = 43.26    #
#         Epoch   9 Time      6.4 lr = 0.002507 avg loss = 2.011913 accuracy = 44.79    #
#         Epoch  10 Time      6.6 lr = 0.001179 avg loss = 1.987894 accuracy = 45.97    #
#         Epoch  11 Time      6.4 lr = 0.000311 avg loss = 1.977104 accuracy = 46.31    #
#         Epoch  12 Time      6.6 lr = 0.000010 avg loss = 1.974445 accuracy = 46.33    #
#                                                                                       #
#         BATCH_SIZE = 128:                                                             #
#         Epoch   0 Time      5.9 lr = 0.000100 avg loss = 2.371592 accuracy =  6.73    #
#         Epoch   1 Time      6.0 lr = 0.003400 avg loss = 2.353004 accuracy =  6.45    #
#         Epoch   2 Time      5.8 lr = 0.006700 avg loss = 2.339738 accuracy =  6.65    #
#         Epoch   3 Time      5.7 lr = 0.010000 avg loss = 2.336695 accuracy =  6.92    #
#         Epoch   4 Time      6.5 lr = 0.009699 avg loss = 2.334418 accuracy =  7.18    #
#         Epoch   5 Time      7.8 lr = 0.008831 avg loss = 2.332328 accuracy =  7.39    #
#         Epoch   6 Time      5.7 lr = 0.007502 avg loss = 2.330570 accuracy =  7.43    #
#         Epoch   7 Time      5.8 lr = 0.005872 avg loss = 2.329191 accuracy =  7.63    #
#         Epoch   8 Time      5.7 lr = 0.004138 avg loss = 2.328188 accuracy =  7.77    #
#         Epoch   9 Time      5.8 lr = 0.002507 avg loss = 2.327525 accuracy =  7.80    #
#         Epoch  10 Time      5.7 lr = 0.001179 avg loss = 2.327147 accuracy =  7.80    #
#         Epoch  11 Time      5.7 lr = 0.000311 avg loss = 2.326978 accuracy =  7.84    #
#         Epoch  12 Time      5.9 lr = 0.000010 avg loss = 2.326935 accuracy =  7.83    #
#                                                                                       #
#########################################################################################

import os.path
import urllib.request
import gzip
import time
import math
import numpy             as np
import matplotlib.pyplot as plt
from google.colab import files

# data
DATA_NUM_TRAIN         = 60000
DATA_NUM_TEST          = 10000
DATA_CHANNELS          = 1
DATA_ROWS              = 28
DATA_COLS              = 28
DATA_CLASSES           = 10
DATA_NORM              = np.float32(255.0)
DATA_URL_TRAIN_DATA    = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'
DATA_URL_TRAIN_LABELS  = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'
DATA_URL_TEST_DATA     = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'
DATA_URL_TEST_LABELS   = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'
DATA_FILE_TRAIN_DATA   = 'train_data.gz'
DATA_FILE_TRAIN_LABELS = 'train_labels.gz'
DATA_FILE_TEST_DATA    = 'test_data.gz'
DATA_FILE_TEST_LABELS  = 'test_labels.gz'
BATCH_SIZE = 1
REGULARIZATION = 0

# model
MODEL_N0 = DATA_ROWS*DATA_COLS
MODEL_N1 = 100
MODEL_N2 = 100
MODEL_N3 = DATA_CLASSES
TOTAL_LAYER = 4
HIDDEN_LAYER = 3 #Last layer is the softmax, 4-3 = 1 is the first layer which is the input

# training
TRAIN_LR_MAX          = 0.01
TRAIN_LR_INIT_SCALE   = 0.01
TRAIN_LR_FINAL_SCALE  = 0.001
TRAIN_LR_INIT_EPOCHS  = 3
TRAIN_LR_FINAL_EPOCHS = 10
TRAIN_NUM_EPOCHS      = TRAIN_LR_INIT_EPOCHS + TRAIN_LR_FINAL_EPOCHS
TRAIN_LR_INIT         = TRAIN_LR_MAX*TRAIN_LR_INIT_SCALE
TRAIN_LR_FINAL        = TRAIN_LR_MAX*TRAIN_LR_FINAL_SCALE

# display
DISPLAY_ROWS   = 8
DISPLAY_COLS   = 4
DISPLAY_COL_IN = 10
DISPLAY_ROW_IN = 25
DISPLAY_NUM    = DISPLAY_ROWS*DISPLAY_COLS

#################################################################################
#                                                                               #
# DATA                                                                          #
#                                                                               #
#################################################################################

# download data
# opener = urllib.request.URLopener()
# opener.addheader('User-Agent', 'Mozilla/5.0')
# if (os.path.exists(DATA_FILE_TRAIN_DATA)   == False):
#     opener.retrieve(DATA_URL_TRAIN_DATA,   DATA_FILE_TRAIN_DATA)
# if (os.path.exists(DATA_FILE_TRAIN_LABELS) == False):
#     opener.retrieve(DATA_URL_TRAIN_LABELS, DATA_FILE_TRAIN_LABELS)
# if (os.path.exists(DATA_FILE_TEST_DATA)    == False):
#     opener.retrieve(DATA_URL_TEST_DATA,    DATA_FILE_TEST_DATA)
# if (os.path.exists(DATA_FILE_TEST_LABELS)  == False):
#     opener.retrieve(DATA_URL_TEST_LABELS,  DATA_FILE_TEST_LABELS)

files.upload()  

# training data
# unzip the file, skip the header, read the rest into a buffer and format to NCHW
file_train_data   = gzip.open("/content/train_data.gz", 'r')
file_train_data.read(16)
buffer_train_data = file_train_data.read(DATA_NUM_TRAIN*DATA_ROWS*DATA_COLS)
train_data        = np.frombuffer(buffer_train_data, dtype=np.uint8).astype(np.float32)
train_data        = train_data.reshape(DATA_NUM_TRAIN, 1, DATA_ROWS, DATA_COLS)

# training labels
# unzip the file, skip the header, read the rest into a buffer and format to a vector
file_train_labels   = gzip.open("/content/train_labels.gz", 'r')
file_train_labels.read(8)
buffer_train_labels = file_train_labels.read(DATA_NUM_TRAIN)
train_labels        = np.frombuffer(buffer_train_labels, dtype=np.uint8).astype(np.int32)

# testing data
# unzip the file, skip the header, read the rest into a buffer and format to NCHW
file_test_data   = gzip.open("/content/test_data.gz", 'r')
file_test_data.read(16)
buffer_test_data = file_test_data.read(DATA_NUM_TEST*DATA_ROWS*DATA_COLS)
test_data        = np.frombuffer(buffer_test_data, dtype=np.uint8).astype(np.float32)
test_data        = test_data.reshape(DATA_NUM_TEST, 1, DATA_ROWS, DATA_COLS)

# testing labels
# unzip the file, skip the header, read the rest into a buffer and format to a vector
file_test_labels   = gzip.open("/content/test_labels.gz", 'r')
file_test_labels.read(8)
buffer_test_labels = file_test_labels.read(DATA_NUM_TEST)
test_labels        = np.frombuffer(buffer_test_labels, dtype=np.uint8).astype(np.int32)

# debug
# print(train_data.shape)   # (60000, 1, 28, 28)
# train_data is a sparse matrix
# reshape function flattens 1*28*28 to a 1
# print(np.reshape(train_data[1,:,:,:],MODEL_N0)/DATA_NORM)
# print(train_data[2,:, :, :])
# print(type(train_data))
print(train_labels.shape[0]) # (60000,)
print(train_labels)
# print(test_data.shape)    # (10000, 1, 28, 28)
# print(test_labels.shape)  # (10000,)

#################################################################################
#                                                                               #
# DATA LOADER                                                                   #
#                                                                               #
#################################################################################

# data loader class
class DataLoader:

    # save images x, labels y and data normalization factor x_norm
    def __init__(self, x, y, x_norm):
      self.x = x
      self.y = y
      self.x_norm = x_norm
        
    # return normalized image t and label t
    def get(self, t, normalize = False):
      image = np.reshape(self.x[t, :, :, :], MODEL_N0)/DATA_NORM
      if normalize == True:
        mu = image[np.nonzero(image)].mean()
        v = image[np.nonzero(image)].var()
        nz = image != 0.0
        image[nz] = (image[nz]-mu)/v     
      label = self.y[t]
      return image, label
       
    # return the total number of images
    def num(self):
      return self.y.shape[0]
# data loaders
data_loader_train = DataLoader(train_data, train_labels, DATA_NORM)
data_loader_test  = DataLoader(test_data,  test_labels,  DATA_NORM)

#################################################################################
#                                                                               #
# LAYERS                                                                        #
#                                                                               #
#################################################################################

# vector matrix multiplication layer
class VectorMatrixMultiplication:

    # initialize input x, parameters h and parameter gradient de/dh
    def __init__(self, x_channels, y_channels):
      self.x = np.zeros(x_channels, dtype = np.float32)
      self.w = np.sqrt(2/(x_channels + y_channels), dtype = np.float32)*np.random.standard_normal((x_channels, y_channels)).astype(np.float32)
      self.dedh = np.zeros(y_channels, dtype = np.float32)

    # save the input x and return y = f(x, h)
    def forward(self, x):
      self.x = x
      return self.x.dot(self.w)
      
    # compute and save the parameter gradient de/dh and return the input gradient de/dx = de/dy * dy/dx
    def backward(self, dedy):
      self.dedh = np.outer(self.x, dedy)
      return dedy.dot(self.w.T)



# vector vector addition layer
class VectorVectorAddition:

    # initialize parameters h and parameter gradient de/dh
    def __init__(self, x_channels):
      self.w = np.zeros(x_channels, dtype = np.float32)
      self.dedh = np.zeros(x_channels, dtype = np.float32)

    # return y = f(x, h)
    def forward(self, x):
      return x + self.w

    # compute and save the parameter gradient de/dh and return the input gradient de/dx = de/dy * dy/dx
    def backward(self, dedy):
      self.dedh = dedy
      return np.copy(self.dedh)
        

# ReLU layer
class ReLU:

    # initialize input x
    def __init__(self, x_channels):
      self.x = np.zeros(x_channels, dtype = np.float32)

    # save the input x and return y = f(x, h)
    def forward(self, x):
      self.x = x
      return np.maximum(np.float32(0.0), self.x)

    # return the input gradient de/dx = de/dy * dy/dx
    def backward(self, dedy):
      return np.minimum(np.float32(1.0), np.maximum(np.float32(0.0),self.x))*dedy


# soft max cross entropy layer
class SoftMaxCrossEntropy:

    # initialize probability p and label
    def __init__(self, y_channels):
      self.p = np.zeros(y_channels, dtype = np.float32)
      self.label = 0
    
    # save the label, compute and save the probability p and return e = f(label, y)
    def forward(self, label, y):
      self.label = label
      z = np.exp(y)
      self.p = z/np.sum(z)
      e = -np.log(self.p[self.label])
      #print(e.shape, e)
      return e
  
    # compute and return the input gradient de/dx from the saved probability and label; e is not used
    def backward(self, e):
      dedy = np.copy(self.p)
      dedy[self.label] = self.p[self.label]-np.float32(1.0)
      return dedy

#################################################################################
#                                                                               #
# NETWORK                                                                       #
#                                                                               #
#################################################################################

# network
class Network:

    # save the network description parameters and create all layers
    def __init__(self, rows, cols, n0, n1, n2, n3, total):
      self.rows = rows
      self.cols = cols
 
      self.n0 = n0
      self.n1 = n1
      self.n2 = n2
      self.n3 = n3
      self.h1 = VectorMatrixMultiplication(self.n0, self.n1)
      self.h2 = VectorVectorAddition(self.n1)
      self.h3 = ReLU(self.n1)
      self.h4 = VectorMatrixMultiplication(self.n1, self.n2)
      self.h5 = VectorVectorAddition(self.n2)    
      self.h6 = ReLU(self.n2)
      self.h7 = VectorMatrixMultiplication(self.n2, self.n3)    
      self.h8 = VectorVectorAddition(self.n3)
      #for i in range(total):
     
      
    # connect layers forward functions together to map the input image to the network output
    # return the network output
    def forward(self, img):
      y1 = self.h1.forward(img)
      y2 = self.h2.forward(y1)
      y3 = self.h3.forward(y2)
      y4 = self.h4.forward(y3)
      y5 = self.h5.forward(y4)
      y6 = self.h6.forward(y5)
      y7 = self.h7.forward(y6)
      y8 = self.h8.forward(y7)
      return y8

      # input = img
      # for i in range(3):
      #   VM = VectorMatrixMultiplication(eval("self.n"+str(i)),eval("self.n"+str(i+1)))
      #   dot = VM.forward(input)
      #   VV = VectorVectorAddition(eval("self.n"+str(i+1)))
      #   addBias = VV.forward(dot)
      #   if i == 2:
      #     return addBias
      #   else:
      #     R = ReLU(eval("self.n"+str(i+1)))
      #     input = R.forward(addBias)

    # connect layers backward functions together to map de/dy at the end of the network to de/dx at the beginning
    # note that inside the backward functions de/dh is computed for all parameters
    # optionally return de/dx (unused)
    def backward(self, dedy):
      dh8_dh7 = self.h8.backward(dedy)  #h8.backward():saved dL_dh8 for w8 in self.dedh and pass back dh8_dh7(dedx7)
      dh7_dh6 = self.h7.backward(dh8_dh7) #h7.backward():saved dh8_dh7 for w7 in self.dedh and pass back dedx6
      dh6_dh5 = self.h6.backward(dh7_dh6) #h6.backward(): pass back dedx5
      dh5_dh4 = self.h5.backward(dh6_dh5) #h5.backward(): saved dh6_dh5 for w5 in self.dedh and pass back dedx4
      dh4_dh3 = self.h4.backward(dh5_dh4)  #h4.backward(): saved dh5_dh4 for w4 in self.dedh and pass back dedx3
      dh3_dh2 = self.h3.backward(dh4_dh3) #h3.backward(): pass back dedx2
      dh2_dh1 = self.h2.backward(dh3_dh2) #h2.backward(): saved dh3_dh2 for w2 in self.dedh and pass back dedx1
      self.h1.backward(dh2_dh1) #h1.backward():saved dh2_dh1 for w1 in self.dedh and pass back dedx0(which is useless)
 
    # update all layers with trainable parameters via h = h - lr * de/dh
    def update(self, lr, total_input_num, regular_penalty = 0):
      self.h1.w = self.h1.w - lr*self.h1.dedh + (regular_penalty*self.h1.w)/total_input_num
      self.h2.w = self.h2.w - lr*self.h2.dedh + (regular_penalty*np.sum(self.h2.w))/total_input_num
      self.h4.w = self.h4.w - lr*self.h4.dedh + (regular_penalty*self.h4.w)/total_input_num
      self.h5.w = self.h5.w - lr*self.h5.dedh + (regular_penalty*np.sum(self.h5.w))/total_input_num 
      self.h7.w = self.h7.w - lr*self.h7.dedh + (regular_penalty*self.h7.w)/total_input_num
      self.h8.w = self.h8.w - lr*self.h8.dedh + (regular_penalty*np.sum(self.h8.w))/total_input_num 
      # self.h1.w = self.h1.w - lr*self.h1.dedh
      # self.h2.w = self.h2.w - lr*self.h2.dedh 
      # self.h4.w = self.h4.w - lr*self.h4.dedh 
      # self.h5.w = self.h5.w - lr*self.h5.dedh 
      # self.h7.w = self.h7.w - lr*self.h7.dedh 
      # self.h8.w = self.h8.w - lr*self.h8.dedh
      #(regular_penalty*self.h1.w)/total_input_num
    # def l2_matrix (w, total_input_num, regular_penalty = 0):
    #   w_o = (regular_penalty*w)/total_input_num
    #   too_small = w_o < 1e-6
    #   w_o[too_small] = 0
    #   return w_o
      

# network
network = Network(DATA_ROWS, DATA_COLS, MODEL_N0, MODEL_N1, MODEL_N2, MODEL_N3, TOTAL_LAYER)

#################################################################################
#                                                                               #
# ERROR                                                                         #
#                                                                               #
#################################################################################

# error
error = SoftMaxCrossEntropy(MODEL_N3)

#################################################################################
#                                                                               #
# UPDATE                                                                        #
#                                                                               #
#################################################################################

# learning rate schedule
def lr_schedule(epoch):

    # linear warmup
    if epoch < TRAIN_LR_INIT_EPOCHS:
        lr = (TRAIN_LR_MAX - TRAIN_LR_INIT)*(float(epoch)/TRAIN_LR_INIT_EPOCHS) + TRAIN_LR_INIT
    # 1/2 wave cosine decay
    else:
        lr = TRAIN_LR_FINAL + 0.5*(TRAIN_LR_MAX - TRAIN_LR_FINAL)*(1.0 + math.cos(((float(epoch) - TRAIN_LR_INIT_EPOCHS)/(TRAIN_LR_FINAL_EPOCHS - 1.0))*math.pi))

    return lr

#################################################################################
#                                                                               #
# TRAIN                                                                         #
#                                                                               #
#################################################################################

# initialize the epoch
start_epoch      = 0
start_time_epoch = time.time()

# initialize the display statistics
epochs   = np.zeros(TRAIN_NUM_EPOCHS, dtype=np.int32)
avg_loss = np.zeros(TRAIN_NUM_EPOCHS, dtype=np.float32)
accuracy = np.zeros(TRAIN_NUM_EPOCHS, dtype=np.float32)

# cycle through the epochs
for epoch in range(start_epoch, TRAIN_NUM_EPOCHS):

    # set the learning rate
    lr = np.float32(lr_schedule(epoch))

    # initialize the epoch statistics
    training_loss   = 0.0
    testing_correct = 0

    # cycle through the training data
    # print(data_loader_train.num()/BATCH_SIZE)
    total_input_number = data_loader_train.num()
    for t in range(0,total_input_number,BATCH_SIZE):
      dedy = np.zeros(10, dtype = np.float32)
      e = 0.0
      if t+BATCH_SIZE > total_input_number:
        batch_iter = total_input_number-t
      else: batch_iter = BATCH_SIZE 
      for b in range(batch_iter):
        # data
        img, label = data_loader_train.get(t+b)
        # dedy = np.zeros(10, dtype = np.float32)
        # network forward pass, error forward pass, error backward pass and network backward pass
        y    = network.forward(img)
        e    += error.forward(label, y)      
        dedy += error.backward(e)
        #input()
      dedx = network.backward(dedy/batch_iter)

        # weight update
      network.update(lr,total_input_number,REGULARIZATION)

        # update statistics
      training_loss = training_loss + e

    # cycle through the testing data
    for t in range(data_loader_test.num()):

        # data
        img, label = data_loader_test.get(t)

        # network forward pass and prediction
        y          = network.forward(img)
        prediction = (np.argmax(y)).astype(np.int32)

        # update statistics
        if (label == prediction):
            testing_correct = testing_correct + 1
            
    # epoch statistics
    elapsed_time_epoch = time.time() - start_time_epoch
    start_time_epoch   = time.time()
    epochs[epoch]      = epoch
    avg_loss[epoch]    = training_loss/data_loader_train.num()
    accuracy[epoch]    = 100.0*testing_correct/data_loader_test.num()
    print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f} accuracy = {4:5.2f}'.format(epoch, elapsed_time_epoch, lr, avg_loss[epoch], accuracy[epoch]), flush=True)

#################################################################################
#                                                                               #
# DISPLAY                                                                       #
#                                                                               #
#################################################################################

# plot of loss and accuracy vs epoch
fig1, ax1 = plt.subplots()
ax1.plot(epochs, avg_loss, color='red')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Avg loss', color='red')
ax1.set_title('Avg Loss And Accuracy Vs Epoch')
ax2 = ax1.twinx()
ax2.plot(epochs, accuracy, color='blue')
ax2.set_ylabel('Accuracy %', color='blue')

# initialize the display predictions
predictions = np.zeros(DISPLAY_NUM, dtype=np.int32)

# cycle through the display data
for t in range(DISPLAY_NUM):

    # data
    img, label = data_loader_test.get(t)

    # network forward pass and prediction
    y              = network.forward(img)
    predictions[t] = (np.argmax(y)).astype(np.int32)

# plot of display examples
fig = plt.figure(figsize=(DISPLAY_COL_IN, DISPLAY_ROW_IN))
ax  = []
for t in range(DISPLAY_NUM):
    img, label = data_loader_test.get(t)
    img        = img.reshape((DATA_ROWS, DATA_COLS))
    ax.append(fig.add_subplot(DISPLAY_ROWS, DISPLAY_COLS, t + 1))
    ax[-1].set_title('True: ' + str(label) + ' xNN: ' + str(predictions[t]))
    plt.imshow(img, cmap='Greys')

# show figures
plt.show()